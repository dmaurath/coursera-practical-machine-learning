---
title: 'Exercise Prediction with Random Forest'
author: "Daniel Maurath"
date: "June 16, 2015"
output: html_document
---



##Executive Summary
The goal of this project for the Coursera Practrical Machine Learning course was to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to predict the manner in which they did the exercise. 

A random forests approach was used and resulted in a model with an prediction accuracy of: 

Expected out of sample error is:


#Data 
The data was provided by researchers at  Pontifical Catholic University of Rio de Janeiro, Informatics Department and School of Computing and Communications, Lancaster University, UK. 

Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6
http://groupware.les.inf.puc-rio.br/work.jsf?p1=10335


#Required Libraries
```{r}
library(caret)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

#Data Cleaning
The dataset contained some columns with many missing values. These columns were summaries for time windows, so I discarded them. I also removed "bookeeping" data in the first 7 columns. These columns were unique to this dataset and would not be useuful when trying to make predictions out of sample.

```{r}
pml <- read.csv('pml-training.csv', na.strings=c("#DIV/0!", ""))

cs <- colSums(is.na(pml))
cs <- data.frame(col = names(cs), countNA = cs)
cs <- cs[order(-cs$countNA),]
row.names(cs) <- 1:nrow(cs)
head(cs,40)
kp <- as.character(cs[cs$countNA <= 0, "col"])
pml <- pml[kp]

pml <- pml[,-c(1:7)]
```

To cross validate, I split the dataset into 60% train and 40% test. I leave the testing set alone for now, and build my features.
```{r}
set.seed(1987)
inTrain <- createDataPartition(y=pml$classe, p=0.60, list=FALSE)
training <- pml[inTrain,]
testing <- pml[-inTrain,]
```

###Feature Building
To reduce noise in the model, I remove predictors that have near zero variance using the nearZeroVar function in caret. I then remove any columns still in my testing set that are not in my training set. 
```{r}
nzv <- nearZeroVar(training, saveMetrics=TRUE)
training <- training[,nzv$nzv==FALSE]
nzv <- nearZeroVar(testing,saveMetrics=TRUE)
testing <- testing[,nzv$nzv==FALSE]

testing <- testing[colnames(training)]
````

Another way to reduce noise, is to use Principal Components Analysis (PCA). This approach finds the best weighting of features. Here you can see many predictors are correlated, meaning this is a good case for PCA. I will use PCA when I train my model.
```{r}
M <- abs(cor(training[,-c(53)]))
diag(M) <- 0
which(M > 0.8, arr.ind=T)
```


#Random Forests Model
I chose Random Forests for my model because of its accuracy. As noted in Week 3's Random Forest's lecture, slide 10, "Random Forests are usually one of the top two performing algorithims" 

Admittedly, there are shortcomings to this model, but I addressed each. Random Forests is slow, so I used the "doParalell" package to take advantage of my computer's 4 cores. Intrepretability was not required for this assignment, only predictive accuracy. I used 3-fold cross-validation to address overfitting as was suggested in lecture. 

I trained the model on the training dataset, tested it using the testing dataset, and then generate a confusion matrix. 
```{r}
modelfitRF <- train(training$classe~., method = "rf", preProcess=c("pca"), trControl = trainControl(method = "cv", number=3), data=training)
cm <- confusionMatrix(testing$classe, predict(modelfitRF, testing))
cm
```
The results are quite good with an accuracy of and kappa of

Out of sample error is 

A more intuitive way to visualize model fit is by plotting the confusion matrix. Here you can it most accurate for Class A, and least for Class C. 

```{r}
cm_plot <- as.data.frame(cm[2])
names(cm_plot) <- c("Predicted", "Actual", "Freq")
plot <- ggplot(cm_plot)
plot + geom_tile(aes(x=Actual, y=Predicted, fill=Freq)) + scale_x_discrete(name="Actual Class") + scale_y_discrete(name="Predicted Class") + scale_fill_gradient() + labs(fill="Normalized\nFrequency")
```




